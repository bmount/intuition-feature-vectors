{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aec994c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This allows us to read the cities.json file. The format of the file\n",
    "# is 'Javascript Object Notation' (JSON). It's a way of representing\n",
    "# essential data structures, like lists and key-value pairs.\n",
    "\n",
    "import json\n",
    "\n",
    "# Our data will be in this format:\n",
    "#    {\n",
    "#        \"city\": \"Beloit\", \n",
    "#        \"growth_from_2000_to_2013\": \"2.9%\", \n",
    "#        \"latitude\": 42.5083482, \n",
    "#        \"longitude\": -89.03177649999999, \n",
    "#        \"population\": \"36888\", \n",
    "#        \"rank\": \"999\", \n",
    "#        \"state\": \"Wisconsin\"\n",
    "#    },\n",
    "\n",
    "cities_dataset = json.load(open('cities.json'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b76bdb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def manhattan_distance(starting_point, destination):\n",
    "    \"\"\"\n",
    "    This is a function that tells you basically, 'If my coordinates were city blocks, and\n",
    "    I couldn't walk through the middle of the block, how far would I have to walk to get from\n",
    "    a to b, using city streets?\n",
    "    \"\"\"\n",
    "    distance = 0\n",
    "    n_dimensions = len(starting_point)\n",
    "    for dimension in range(n_dimensions):\n",
    "        # Could be streets, could be avenues, could be anything!\n",
    "        distance_at_dimension = abs(starting_point[dimension] - destination[dimension])\n",
    "        distance += distance_at_dimension\n",
    "    return distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0886a983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compare cities!\n",
    "\n",
    "# First, we know that cities close together have some things in common.\n",
    "# It's fair to say that Chicago and Minneapolis are 'similar' in some\n",
    "# important ways. Let's see what computing the overlap of latitudes and\n",
    "# longitudes looks like.\n",
    "\n",
    "# We're going to store the cities in a dictionary\n",
    "# The key will be the city name\n",
    "# The value will be the coordinates of the city. Latitude and longitude\n",
    "# are just a geographic, 2 dimensional vector:\n",
    "\n",
    "lonlat = {}\n",
    "\n",
    "for city in cities_dataset:\n",
    "    lonlat[city['city']] = [city['longitude'], city['latitude']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b003020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a205032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Woah! That's a lot of cities! Let's simplify with ones with a bigger population:\n",
    "big_cities = {}\n",
    "for city in cities_dataset:\n",
    "    if int(city['population']) > 100000:\n",
    "        big_cities[city['city']] = [city['longitude'], city['latitude']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e594d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6734cf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's make a function that finds the nearest city to each other city,\n",
    "# using the oversimplified-but-fine approximation of the Manhattan distance.\n",
    "# We'll also take a little shortcut, in that extremely close cities are often\n",
    "# really more of the same city (Houston-Pearland, San Francisco-Daly City). For\n",
    "# cities to even be distinct at all geographically, let's say they have to be\n",
    "# roughly 1,000 blocks apart (0.001 degree longitude is about a block in North\n",
    "# America, appropriate for our Manhattan distance metric ;)\n",
    "def approximate_nearest_city(city_name, cities, minimum_distance = 1.0):\n",
    "    coordinates = cities[city_name]\n",
    "    # So far, we have nothing 'nearest', so start our search with a large value:\n",
    "    nearest = 1000\n",
    "    nearest_name = None\n",
    "    for other_city, other_coordinates in cities.items():\n",
    "        # calculate the distance between the two cities\n",
    "        distance = manhattan_distance(coordinates, other_coordinates)\n",
    "        # if the distance is less than the current nearest, update the nearest\n",
    "        # unless the distance is so small we'd really be comparing the same city\n",
    "        # to itself\n",
    "        if distance < nearest and distance > minimum_distance:\n",
    "            nearest = distance\n",
    "            nearest_name = other_city\n",
    "\n",
    "    # return the nearest city\n",
    "    return nearest_name, nearest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cc5d903b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the nearest city to New York is Bridgeport (1.2845287000000098)\n",
      "the nearest city to Los Angeles is San Bernardino (1.010030399999998)\n",
      "the nearest city to Chicago is Milwaukee (1.437464300000002)\n",
      "the nearest city to Houston is Beaumont (1.5629939000000057)\n",
      "the nearest city to Philadelphia is Elizabeth (1.665928600000008)\n",
      "It took 0.0031671524047851562 seconds to find the nearest city to each other city.\n"
     ]
    }
   ],
   "source": [
    "# Now, let's test it out. Also, we are going to see how long it takes to find\n",
    "# the nearest city to each other city:\n",
    "import time\n",
    "starting_time = time.time()\n",
    "for city in list(big_cities.keys())[:5]:\n",
    "    nearest, how_far = approximate_nearest_city(city, big_cities)\n",
    "    print(f'the nearest city to {city} is {nearest} ({how_far})')\n",
    "\n",
    "ending_time = time.time()\n",
    "\n",
    "print('It took', ending_time - starting_time, 'seconds to find the nearest city to each other city.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383a8891",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be58b923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So this kind of works, but we want something a little deeper. More 'similarity' than just\n",
    "# 'nearness'. Let's try to find the most similar city to each other city, using additional features.\n",
    "# Intuitively, nearby cities do share a lot in common. But also, big cities have some of that bigness\n",
    "# in common, apart from geography. Let's see what we can do with that.\n",
    "\n",
    "# Now, let's start thinking of our cities list as a matrix. Each row is a city, and each column is a feature.\n",
    "big_cities_list = [city for city in cities_dataset if int(city['population']) > 100000]\n",
    "names = [city['city'] for city in big_cities_list]\n",
    "longitudes = [city['longitude'] for city in big_cities_list]\n",
    "latitudes = [city['latitude'] for city in big_cities_list]\n",
    "populations = [float(city['population']) for city in big_cities_list]\n",
    "growth_rates = [float(city['growth_from_2000_to_2013'].replace('%', '')) for city in big_cities_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bb6e3c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So far, so good, but how do we compare the number of people in one city to the degrees latitude of another?\n",
    "# Answer: you don't! Intuitively, we know that longitude, the Eastness-Westness of a city, does mean something\n",
    "# to us. Seattle has some things in common with San Francisco much further South. And likewise Phoenix to\n",
    "# Jacksonville in terms of latitude -- there's a real \"sunbelt\" effect uniting these distant cities. So we want\n",
    "# to keep all those features, but not over-emphasize any in particular. Again, we're designing our features here.\n",
    "# And BTW, since we're designing features, let's throw in one more: the market share of pickup trucks in the state!\n",
    "# Because why not?! Also lets us practice reading CSV files.\n",
    "\n",
    "# our format will be:\n",
    "# Connecticut,9.4%\n",
    "# New Jersey,7.8%\n",
    "# ...\n",
    "\n",
    "import csv\n",
    "share = csv.DictReader(open('pickup-trucks.csv'), delimiter = ',', fieldnames=['state', 'market_share'])\n",
    "state_truck_share = {}\n",
    "for state in share:\n",
    "    state_name = state['state']\n",
    "    market_share = float(state['market_share'].replace('%', ''))\n",
    "    state_truck_share[state_name] = market_share\n",
    "\n",
    "pickup_market_share = [state_truck_share[city['state']] for city in big_cities_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1f59b5c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11.4, 10.3, 10.8, 19.0, 13.6]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickup_market_share[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a2216c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back to our matrix! We definitely won't be comparing longitudes to pickups! So let's rescale all our features\n",
    "\n",
    "# We'll use this common function:\n",
    "\n",
    "def rescale(vector, range = (-0.5, 0.5)):\n",
    "    \"\"\"\n",
    "    This function takes a vector of numbers, and rescales it to be between\n",
    "    the range specified. The default range is -0.5 to 0.5, which is where\n",
    "    floating point numbers are most accurate.\n",
    "    \"\"\"\n",
    "    # find the minimum and maximum values in the vector\n",
    "    min_value = min(vector)\n",
    "    max_value = max(vector)\n",
    "    # find the range of the vector\n",
    "    vector_range = max_value - min_value\n",
    "    # find the range we want to rescale to\n",
    "    target_range = range[1] - range[0]\n",
    "    # rescale the vector\n",
    "    rescaled = [range[0] + (x - min_value) * target_range / vector_range for x in vector]\n",
    "    return rescaled\n",
    "\n",
    "# Now, let's rescale all our features:\n",
    "rescaled_longitudes = rescale(longitudes)\n",
    "rescaled_latitudes = rescale(latitudes)\n",
    "rescaled_populations = rescale(populations)\n",
    "rescaled_growth_rates = rescale(growth_rates)\n",
    "rescaled_pickup_market_share = rescale(pickup_market_share)\n",
    "\n",
    "# Now, let's put all our features together into a matrix:\n",
    "import numpy as np\n",
    "\n",
    "matrix = np.array([rescaled_longitudes, rescaled_latitudes, rescaled_populations,\n",
    "    rescaled_growth_rates, rescaled_pickup_market_share])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1826516d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's transpose the matrix, so that each row is a city, and each column is a feature:\n",
    "\n",
    "matrix = matrix.T\n",
    "\n",
    "# Now, let's take a couple of approaches. Here we normalize so that each city's vector has length 1:\n",
    "normalized_matrix = matrix / np.linalg.norm(matrix, axis = 1).reshape(-1, 1)\n",
    "\n",
    "# Working with the normalized matrix is really like \"how much are these two vectors pointing in the same direction?\"\n",
    "# Because the magnitude of each row is 1, we can just take the dot product of each row with every other row. This\n",
    "# is the same as matrix multiplication, but we're multiplying a matrix by its transpose. So we can use the @ operator\n",
    "\n",
    "similarities = normalized_matrix @ normalized_matrix.T\n",
    "\n",
    "# Now, arrange the results such that the most similar city is at the top of the list (which will be itself), and\n",
    "# the least similar city is at the bottom of the list (which will be the city furthest away in terms of our combined\n",
    "# features):\n",
    "\n",
    "# Axis = 1 means we're sorting each row, and -1 means we're sorting in descending order\n",
    "# Argsort returns the indices of the sorted array -- where you would have to put stuff so that it appears in order\n",
    "rankings = np.argsort(-similarities, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "89075d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " New York is most similar to Chicago, Los Angeles, Philadelphia, Boston, Baltimore and least similar to Billings, Boise City, Frisco, Surprise, Anchorage\n",
      " Los Angeles is most similar to San Diego, San Jose, Phoenix, San Francisco, Long Beach and least similar to Sioux Falls, Anchorage, Surprise, Billings, Frisco\n",
      " Chicago is most similar to Baltimore, Philadelphia, Newark, Paterson, Jersey City and least similar to Boise City, Billings, Anchorage, Surprise, Frisco\n",
      " Houston is most similar to San Antonio, Dallas, Austin, Mobile, New Orleans and least similar to Billings, New York, Frisco, Surprise, Anchorage\n",
      " Philadelphia is most similar to Boston, Baltimore, Yonkers, Washington, Virginia Beach and least similar to Billings, Honolulu, Frisco, Surprise, Anchorage\n",
      " Phoenix is most similar to Tucson, Mesa, Glendale, Tempe, Santa Ana and least similar to Billings, Surprise, New York, Anchorage, Frisco\n"
     ]
    }
   ],
   "source": [
    "# Now, let's print out the most similar city to each other city:\n",
    "\n",
    "def show_rankings(rankings, names, n=5, preamble=''):\n",
    "    for index, name in enumerate(names):\n",
    "        if index > 5: # Let's just print the first few\n",
    "            return\n",
    "        top_n = rankings[index][1:1+n]\n",
    "        near = [names[i] for i in top_n]\n",
    "        far = [names[i] for i in rankings[index][-n:]]\n",
    "        print(preamble, name, 'is most similar to', ', '.join(near), 'and least similar to', ', '.join(far))\n",
    "\n",
    "show_rankings(rankings, names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9b1e9fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are some interesting results here. For example, this record:\n",
    "\n",
    "\"\"\"{'city': 'Frisco',\n",
    " 'growth_from_2000_to_2013': '287.7%',\n",
    " 'latitude': 33.1506744,\n",
    " 'longitude': -96.82361159999999,\n",
    " 'population': '136791',\n",
    " 'rank': '186',\n",
    " 'state': 'Texas'}\"\"\"\n",
    "\n",
    "# Frisco, Texas has such an astounding growth rate that it really is very unlike anywhere on the list!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bacd9515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So now we have some intuition about feature vectors. A \"feature\" is just a value like \"population growth rate\",\n",
    "# or latitude, or market share of pickup trucks. We've got a few manually assembled features like that.\n",
    "\n",
    "# Deep learning is capable of finding those features for us. We tell the model, find us the latitude, and frombitude,\n",
    "# and glouflitude... for hundreds of dimensions. Find whatever we need to do our task.\n",
    "\n",
    "# So let's try some deep learning. Instead of making up a feature vector for each city, we let loose\n",
    "# a neural network, with the broad instructions of: \"find the 512 dimensions (*-tudes!) that best capture\n",
    "# the semantic essence of every word and image on the internet\". The below model learns which images go\n",
    "# with which captions -- we'll use it here to do the caption part, independent of any images. We need to\n",
    "# get our data into a format that a neural network can understand -- for this model that just means English!\n",
    "\n",
    "import sentence_transformers\n",
    "model = sentence_transformers.SentenceTransformer('clip-ViT-B-32').to('cuda')\n",
    "\n",
    "def embed_row(row):\n",
    "    # Little bit of practical engineering here: language models are not exceptionally\n",
    "    # good at dealing with numbers. And the really big ones don't entirely need to be!\n",
    "    # So let's just use the proper name for the city an state, and see what happens:\n",
    "    sentence = f\"\"\"{row.get('city')}, {row['state']}\"\"\"\n",
    "    return model.encode(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "07148144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's embed all our cities:\n",
    "embeddings = np.array([embed_row(row) for row in big_cities_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "908bba20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[4].shape\n",
    "# These are just normalized vectors, same as the ones we made up with latitude, trucks, etc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7c784727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting pretty close now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "456f35b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's see how similar these embeddings are to each other:\n",
    "embeddings_distances = embeddings @ embeddings.T\n",
    "embeddings_rankings = np.argsort(-embeddings_distances, axis = 1)\n",
    "\n",
    "\n",
    "def top_n_ranking(rankings, index, n=5):\n",
    "    top_n = rankings[index][1:1+n]\n",
    "    near = [names[i] for i in top_n]\n",
    "    far = [names[i] for i in rankings[index][-n:]]\n",
    "    return near, far\n",
    "\n",
    "\n",
    "def show_ranking(rankings, index, preamble=''):\n",
    "    near, far = top_n_ranking(rankings, index)\n",
    "    print(preamble, 'is most similar to:', ', '.join(near))\n",
    "    print(preamble, 'is least similar to:', ', '.join(far))\n",
    "\n",
    "\n",
    "def compare_models(city_name, state_name):\n",
    "    # Almost 10 of America's top 300 cities share the same name. So we need to be more specific.\n",
    "    idx = 0\n",
    "    found = False\n",
    "    for row in big_cities_list:\n",
    "        if row['city'] == city_name and row['state'] == state_name:\n",
    "            found = True\n",
    "            break\n",
    "        idx += 1\n",
    "    if not found:\n",
    "        print(\"Couldn't find\", city_name, 'in', state_name)\n",
    "        return\n",
    "    print('Now find the most similar city to', city_name, 'in each model:')\n",
    "    show_ranking(rankings, idx, preamble=f'With our manually created basic features, {city_name}, {state_name} ')\n",
    "    show_ranking(embeddings_rankings, idx, preamble=f'With our deep learning model, {city_name}, {state_name} ')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e512ad00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now find the most similar city to San Diego in each model:\n",
      "With our manually created basic features, San Diego, California  is most similar to: Long Beach, Santa Ana, Anaheim, Huntington Beach, Oceanside\n",
      "With our manually created basic features, San Diego, California  is least similar to: Surprise, New York, Billings, Anchorage, Frisco\n",
      "With our deep learning model, San Diego, California  is most similar to: Norwalk, Riverside, New York, Fullerton, Hayward\n",
      "With our deep learning model, San Diego, California  is least similar to: Stamford, New Haven, Raleigh, Waterbury, Henderson\n"
     ]
    }
   ],
   "source": [
    "compare_models('San Diego', 'California')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d9e79e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now find the most similar city to Anchorage in each model:\n",
      "With our manually created basic features, Anchorage, Alaska  is most similar to: Boise City, Billings, Portland, Everett, Gresham\n",
      "With our manually created basic features, Anchorage, Alaska  is least similar to: Surprise, Miramar, Port St. Lucie, Frisco, New York\n",
      "With our deep learning model, Anchorage, Alaska  is most similar to: Everett, Anchorage, Thornton, Norwalk, Hayward\n",
      "With our deep learning model, Anchorage, Alaska  is least similar to: Stamford, New Haven, Raleigh, Waterbury, Henderson\n"
     ]
    }
   ],
   "source": [
    "compare_models('Anchorage', 'Alaska')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1799f266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now find the most similar city to Dallas in each model:\n",
      "With our manually created basic features, Dallas, Texas  is most similar to: Mobile, Beaumont, Arlington, Garland, Birmingham\n",
      "With our manually created basic features, Dallas, Texas  is least similar to: Billings, New York, Anchorage, Frisco, Surprise\n",
      "With our deep learning model, Dallas, Texas  is most similar to: Irving, Carrollton, Houston, New York, Frisco\n",
      "With our deep learning model, Dallas, Texas  is least similar to: Charleston, New Haven, Raleigh, Waterbury, Henderson\n"
     ]
    }
   ],
   "source": [
    "compare_models('Dallas', 'Texas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1f0ede51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now find the most similar city to San Francisco in each model:\n",
      "With our manually created basic features, San Francisco, California  is most similar to: San Jose, Oakland, Sacramento, Daly City, Fremont\n",
      "With our manually created basic features, San Francisco, California  is least similar to: Anchorage, Surprise, Billings, New York, Frisco\n",
      "With our deep learning model, San Francisco, California  is most similar to: San Mateo, Norwalk, Santa Ana, San Jose, Hayward\n",
      "With our deep learning model, San Francisco, California  is least similar to: Bridgeport, New Haven, Raleigh, Henderson, Waterbury\n"
     ]
    }
   ],
   "source": [
    "compare_models('San Francisco', 'California')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2144336",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
